{"topic":{"0":"N\/A","1":"N\/A","2":"N\/A","3":"N\/A","4":"N\/A","5":"N\/A"},"dialog_lvl":{"0":"eli5","1":"eli5","2":"eli5","3":"eli5","4":"eli5","5":"eli5"},"role":{"0":"Explainee","1":"Explainer","2":"Explainee","3":"Explainer","4":"Explainee","5":"Explainer"},"turn_num_tokens":{"0":18,"1":185,"2":11,"3":39,"4":12,"5":17},"turn":{"0":"ELI5: what Apple\u2019s child-exploitation prevention efforts do, and what information is sent where, how, and under what circumstances.","1":"Well, Apple will have the data, on your phone.  They describe everything as though its based on \"when you upload your picture to iCloud\".  That's not to say they couldn't, but it would certainly consume data unexpectedly.  The advantage of the current description is that if you're already uploading a 12MP image, a few hundred extra bytes of CSAM prevention data is in the noise.  If Apple has it, then the court can make them give it to lots of organizations.  That's potentially the big problem, if some court orders Apple to use this tech to find out if you have other pictures, perhaps copyright infringing (or government protesting in some countries), then Apple won't be able to say \"That would be impossible to do\".  Once you have enough \"matches\" then Apple will be able to decode all the photos in your account that \"match\".  What they do with those unencrypted images is limited only by your imagination.  If it's a \"false match\", it's almost certain to go into their training dataset.  Again, courts could order any random old thing.","2":"Can\u2019t help but notice that wasn\u2019t a yes or no answer","3":"It's impossible to know what Apple will do.  I'm pretty deeply involved in assessment of the crypto aspects of this, but I don't work for Apple.  If I was, they wouldn't let me answer things on Reddit.","4":"Could you answer just based off of the current information they gave?","5":"Yes, if they think you are a child abuser.  No, except for internal uses like improving the system."}}